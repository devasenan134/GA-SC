{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":903,"status":"ok","timestamp":1671105990896,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"7ww5gjz_B-yc","outputId":"5abf9e18-cfeb-4231-ea76-9f55d9bc6e0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n","Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/devasenan/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/devasenan/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/devasenan/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     /Users/devasenan/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"izg9ldpd-E7i"},"source":["# **1. Pre-Processing**\n","1. Tokenization\n","2. Stemming/lemmatization\n","3. Bow/TF-IDF "]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671106094950,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"5o5kvRDQ8Wgf"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","import re\n","import numpy as np\n","\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671106094950,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"JABw2Ms1-EPn"},"outputs":[],"source":["def tokenize_lemmatizor(frame):\n","    words = []\n","    lemma_words = []\n","    lemma_sentences = []\n","    lemmatizer = WordNetLemmatizer()\n","\n","    for i in range(len(frame)):\n","        words = nltk.word_tokenize(frame.iloc[i])\n","        lemma_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n","        lemma_sentences.append(\" \".join(lemma_words))\n","\n","    return lemma_sentences\n","\n","\n","def re_lemmatizor(frame):\n","    lemmatizer = WordNetLemmatizer()\n","    review = []\n","    corpus = []\n","\n","    for i in range(len(frame)):\n","        review = re.sub('[^a-zA-Z]', ' ', frame.iloc[i])\n","        review = review.lower()\n","        review = review.split()\n","        # these lines represent - words = nltk.word_tokenize(frame.cmd[i])\n","\n","        review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n","        # lemma_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))])\n","        \n","        corpus.append(\" \".join(review))\n","        # lemma_sentences.append(\" \".join(lemma_words))\n","\n","    return corpus"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YPogbiu7QdPm"},"source":["### 3.1. BOW"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671106094950,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"-CMLKB7rRO5m"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer # bow\n","def tokens_to_bow(corpus_tokens, tokenizer=1):\n","    cv = CountVectorizer(max_features=5000)\n","    tokens = []\n","    if tokenizer == 1:\n","        tokens = tokenize_lemmatizor(corpus_tokens)\n","        X_bow = cv.fit_transform(tokens).toarray()\n","    else:\n","        tokens = re_lemmatizor(corpus_tokens)\n","        X_bow = cv.fit_transform(tokens).toarray()\n","    features = cv.get_feature_names_out()\n","    return X_bow, features"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6UuM5nekVuo5"},"source":["### 3.2. TF-IDF"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671106094951,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"5KBzYDsxV124"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer # tfidf\n","def tokens_to_tfidf(corpus_tokens, tokenizer=1):\n","    tfidf = TfidfVectorizer()\n","    tokens = []\n","    if tokenizer:\n","        tokens = tokenize_lemmatizor(corpus_tokens)\n","        X_tfidf = tfidf.fit_transform(tokens).toarray()\n","    else:\n","        tokens = re_lemmatizor(corpus_tokens)\n","        X_tfidf = tfidf.fit_transform(tokens).toarray()\n","    return X_tfidf, tokens"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9oEtXvs7jTJo"},"source":["## **Implementation**"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1671106092820,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"qatoafFKi_nY"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import time\n","from random import randint, choices, randrange, random, sample, shuffle\n","\n","\n","from sklearn import svm\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","from sklearn.naive_bayes import MultinomialNB\n","\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.model_selection import KFold, cross_val_score"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1671106092822,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"LzrikaHziawb"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","def split(df,label):\n","    X_tr, X_te, Y_tr, Y_te = train_test_split(df, label, test_size=0.25, random_state=42)\n","    return X_tr, X_te, Y_tr, Y_te"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1671106092822,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"RXX7NqZjhzne"},"outputs":[],"source":["classifiers = ['LinearSVM', 'RadialSVM', \n","               'Logistic',  'RandomForest', \n","               'DecisionTree', 'KNeighbors',\n","               'MultinomialNB']\n","\n","models = [svm.SVC(kernel='linear'),\n","          svm.SVC(kernel='rbf'),\n","          LogisticRegression(max_iter = 1000),\n","          RandomForestClassifier(n_estimators=200, random_state=0),\n","          DecisionTreeClassifier(random_state=0),\n","          KNeighborsClassifier(),\n","          MultinomialNB()]"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671106133007,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"HtqwQ-jziVs5"},"outputs":[],"source":["def acc_score(df,label):\n","    Score = pd.DataFrame({\"Classifier\":classifiers})\n","    j = 0\n","    acc = []\n","    exec_time = []\n","    X_train,X_test,Y_train,Y_test = split(df,label)\n","    for i in models:\n","        model = i\n","\n","        st = time.time()\n","        model.fit(X_train,Y_train)\n","        et = time.time()\n","\n","        predictions = model.predict(X_test)\n","        acc.append(accuracy_score(Y_test,predictions))\n","        exec_time.append(et-st)\n","        j = j+1     \n","    Score[\"Accuracy\"] = acc\n","    Score['Exec_Time_secs'] = exec_time\n","    Score.sort_values(by=\"Accuracy\", ascending=False,inplace = True)\n","    Score.reset_index(drop=True, inplace=True)\n","    return Score"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def initial_population_term_selection_idf(idf, idf_threshold):\n","    selected_indexes = []\n","    selected_terms = []\n","    idf_dict = dict(idf)\n","    for word, idf in idf_dict.items():\n","        if idf <= idf_threshold:\n","            selected_terms.append(word)\n","            selected_indexes.append(all_terms.index(word))\n","    return selected_indexes, selected_terms"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["len(initial_population_term_selection_idf(idf, 2.6)[1])\n","418\n","len(initial_population_term_selection_idf(idf, 2.7)[1])\n","651\n","len(initial_population_term_selection_idf(idf, 2.8)[1])\n","651\n","len(initial_population_term_selection_idf(idf, 2.9)[1])\n","651\n","len(initial_population_term_selection_idf(idf, 3)[1])\n","1553"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":544,"status":"ok","timestamp":1671106093348,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"_k_nxi6mEgPr"},"outputs":[],"source":["def generate_chromo(selected_indexes, features_count, chromo_size):\n","    features = sample(selected_indexes, k=features_count)\n","    features.sort()\n","    chromo = [1 if i in features else 0 for i in range(chromo_size)]\n","    return np.array(chromo)\n","\n","def generate_population(size, features_count, chromo_size, selected_indexes):\n","    return [generate_chromo(selected_indexes, features_count, chromo_size) for _ in range(size)]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def fitness_score(population):\n","    scores = []\n","    for chromosome in population:\n","        features = np.where(chromosome!=0)[0]\n","        logmodel.fit(X_train[:,features],Y_train)         \n","        predictions = logmodel.predict(X_test[:,features])\n","        scores.append(accuracy_score(Y_test,predictions))\n","    scores, population = np.array(scores), np.array(population) \n","    inds = np.argsort(scores)                                    \n","    return list(scores[inds][::-1]), list(population[inds,:][::-1])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def de_fitness(chromo_set):\n","    max_score = 0\n","    best_chromo = None\n","\n","    for chromo in chromo_set:\n","        features = np.where(chromo!=0)[0]\n","        logmodel.fit(X_train[:,features],Y_train)         \n","        predictions = logmodel.predict(X_test[:,features])\n","        score =  accuracy_score(Y_test,predictions)\n","        if max_score < score:\n","            max_score = score\n","            best_chromo = chromo\n","\n","    return max_score, best_chromo                                "]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def de_crossover(parent1, parent2, probability):\n","    child = []\n","    parent_1, parent_2 = parent1.copy(), parent2.copy()\n","    chromo_len = parent_1.shape[0]\n","    \n","    for i in range(chromo_len):\n","        if random() < probability:\n","            child.append(parent_1[i])\n","        else:\n","            child.append(parent_2[i])\n","    child = np.array(child)\n","    \n","    # there is randomization in this part, in future incase of any unexpected results, have to concentrate in this part\n","    features = np.where(child > 0)[0]\n","    non_features = np.where(child <= 0)[0]\n","\n","    # must_have_features = np.intersect1d(features, selected_indexes)\n","    # extra_features = np.setdiff1d(np.union1d(features, non_features), must_have_features)\n","    # # print(len(must_have_features))\n","    \n","    # features_count = len(must_have_features)\n","    # if len(must_have_features) > 100:\n","    #     to_remove = features_count - 100\n","    #     features = np.setdiff1d(must_have_features, sample(list(must_have_features), k=to_remove))\n","    # else:\n","    #     to_add = 100 - features_count\n","    #     features = np.append(must_have_features, sample(list(extra_features), k=to_add))\n","    \n","    features.sort()\n","    new_child = np.array([1 if i in features else 0 for i in range(chromo_len)])\n","    return de_fitness([new_child, parent_1])[1]"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["def de_mutation(pop_after_fit, co_probability, tfidf_threshold, n_parents):\n","    # getting the population size\n","    pop_size = len(pop_after_fit)\n","    \n","    # getting the length of the chromosome\n","    chromo_len = len(pop_after_fit[0])\n","    # print(chromo_len)\n","\n","    # new variable for the mutated population\n","    pop_nextgen = []\n","\n","    # looping throught all the parent chromos in population\n","    for target in range(pop_size):\n","        # sample_space = list(range(pop_size))\n","        # sample_space.remove(target)\n","        \n","        tf_idf_sent_score = term_frequency_inverse_document_frequency(pop_after_fit)\n","        sent_indexes = [i for i, j in tf_idf_sent_score if (j <= tfidf_threshold) and (i != target)]\n","\n","        # sent_indexes = [tf_idf_sent_score[i][0] for i in range(len(tf_idf_sent_score)) if tf_idf_sent_score[i][0] != target]\n","        # sent_indexes = sent_indexes[:30]\n","        # sent_indexes.sort()\n","        # print(sent_indexes)\n","        # random selection of target chromo, and 2 random chromos\n","        rv1, rv2, rv3 = sample(sent_indexes, k=3)\n","        \n","        target_vec = pop_after_fit[target].astype(np.float32)\n","        random_vec1 = pop_after_fit[rv1]\n","        random_vec2 = pop_after_fit[rv2]\n","        random_vec3 = pop_after_fit[rv3]\n","\n","        # performing the DE mutation\n","        trail_vec = random_vec1 + 0.3*(random_vec2-random_vec3)\n","        # x1_features = np.where(random_vec1 > 0)[0]\n","        # x2_features = np.where(random_vec2 > 0)[0]\n","        # x3_features = np.where(random_vec3 > 0)[0]\n","        \n","        # common_features = np.intersect1d(x1_features, x2_features)\n","        # common_features = np.intersect1d(common_features, x3_features)\n","        # print(common_features, len(common_features))\n","        \n","        # print(\"x1\", *random_vec1)\n","        # print(\"x2\", *(random_vec2-random_vec3))\n","        # print(\"u1\", *trail_vec)\n","        # print(len(np.where(trail_vec > 0)[0]))\n","        \n","        features = np.where(trail_vec > 0.5)[0]\n","        non_features = np.where(trail_vec <= 0.5)[0]\n","        \n","        #----------------------#\n","        # vec1_features = np.where(random_vec1 <= 0)[0]\n","        # not_imp_features = np.setdiff1d(features, vec1_features)\n","        \n","        # if len(features) > 100:\n","        #     to_remove = len(features) - 100\n","        #     features = np.setdiff1d(features, sample(list(not_imp_features), k=to_remove))\n","        # elif len(features) < 100:\n","        #     to_add = 100 - len(features)\n","        #     features = np.append(features, s  ample(list(vec1_features), k=to_add))\n","        #----------------------#\n","        \n","        features.sort()\n","        trail_vec = np.array([1 if i in features else 0 for i in range(chromo_len)])\n","        trail_vec = trail_vec.astype(np.int64)\n","        \n","        new_trail = de_crossover(target_vec, trail_vec, co_probability)   \n","        \n","        pop_nextgen.append(new_trail)\n","\n","    return pop_nextgen"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def term_frequency(population):\n","    tf_sent = []\n","    tf_dict = {}\n","    total_no_terms = len(population)\n","    for chromosome in population:\n","        chromo_tf = []\n","        indexes = np.where(chromosome!=0)\n","        for i in indexes[0]:\n","            chromo_tf.append(chromosome[i]/total_no_terms)\n","            tf_dict[all_terms[i]] = tf_dict.get(all_terms[i], 0) + (chromosome[i]/total_no_terms)\n","        tf_sent.append(chromo_tf)\n","    \n","    tf_terms = sorted(tf_dict.items(), key=lambda x: x[1], reverse=True)\n","    return tf_sent, tf_terms"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def inverse_document_frequency(population):\n","    idf = {}\n","    terms = np.array(list(all_terms))\n","    no_documents = len(population)\n","    for i in range(len(all_terms)):\n","        k = 0\n","        for chromosome in population:\n","            indexes = np.where(chromosome!=0)\n","            if terms[i] in terms[indexes]:\n","                k += 1\n","        idf[terms[i]] = np.log10(no_documents/k)\n","    idf = sorted(idf.items(), key=lambda x: x[1], reverse=True)\n","    return idf"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def term_frequency_inverse_document_frequency(population):\n","    tf_sent, tf_terms = term_frequency(population)\n","    tf_idf = {}\n","    idf_dict = dict(idf)\n","    for i in range(len(population)):\n","        tf_idf_sent = []\n","        indexes = np.where(population[i] != 0)[0]\n","        for j in range(len(indexes)):\n","            idf_term = idf_dict[all_terms[indexes[j]]]\n","            tf = tf_sent[i][j]\n","            tf_idf_sent.append(tf*idf_term)\n","        tf_idf[i] = sum(tf_idf_sent)/len(indexes)\n","    tf_idf = sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)\n","    return tf_idf"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1671106093351,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"jlLAoKgGs_to"},"outputs":[],"source":["def evolution(size, features_count, chromo_size,\n","            n_parents,\n","            crossover_pb,\n","            n_gen,\n","            idf, idf_threshold,\n","            tfidf_threshold\n","            ):\n","    best_chromo= []\n","    best_score= []\n","    \n","    selected_indexes, selected_terms = initial_population_term_selection_idf(idf, idf_threshold)\n","    population_nextgen=generate_population(size, features_count, chromo_size, selected_indexes)\n","    # scores, pop_after_fit = fitness_score(population_nextgen)\n","    # population_nextgen = pop_after_fit.copy()\n","\n","    for i in range(n_gen):\n","        scores, pop_after_fit = fitness_score(population_nextgen.copy())\n","        best_chromo.append(pop_after_fit[0])\n","        best_score.append(scores[0])\n","        print('Best score in generation',i+1,':',scores[0], \"feat_count:\", np.where(pop_after_fit[0] != 0)[0].shape)\n","        \n","        population_nextgen = de_mutation(population_nextgen.copy(), crossover_pb, tfidf_threshold, n_parents)\n","        \n","        print(\"Population size:\", len(population_nextgen))\n","        \n","    return best_chromo,best_score"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4RCS1eQ0G2Bn"},"source":["# **Accuracy Comparison**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RFZbHNV6sIVP"},"source":["### Data Preprocessing"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671106100902,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"5Pm8wDKNpdI3","outputId":"2daf50e3-6cc9-4559-b6e3-a222454c43e2"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cmd</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>So there is no way for me to plug it in here i...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Good case, Excellent value.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Great for the jawbone.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Tied to charger for conversations lasting more...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The mic is great.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>The screen does get smudged easily because it ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>What a piece of junk.. I lose more calls on th...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>Item Does Not Match Picture.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>The only thing that disappoint me is the infra...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>You can not answer calls with the unit, never ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                                                   cmd  score\n","0    So there is no way for me to plug it in here i...      0\n","1                          Good case, Excellent value.      1\n","2                               Great for the jawbone.      1\n","3    Tied to charger for conversations lasting more...      0\n","4                                    The mic is great.      1\n","..                                                 ...    ...\n","995  The screen does get smudged easily because it ...      0\n","996  What a piece of junk.. I lose more calls on th...      0\n","997                       Item Does Not Match Picture.      0\n","998  The only thing that disappoint me is the infra...      0\n","999  You can not answer calls with the unit, never ...      0\n","\n","[1000 rows x 2 columns]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["amazon = pd.read_csv(\"../dataset/amazon.csv\")\n","frame = amazon.copy()\n","frame"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1766,"status":"ok","timestamp":1671106102663,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"8vV0bqNbrkxk","outputId":"6707bf88-6b91-4616-8f2a-8eacdb2833d1"},"outputs":[],"source":["X_bow, features = tokens_to_bow(frame.cmd, 0)\n","y_score = frame.score\n","all_terms = list(features)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["X_train, X_test, Y_train, Y_test = split(X_bow, y_score)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["idf = inverse_document_frequency(X_bow)\n","tf_sent, tf_terms = term_frequency(X_bow)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZoErA4fHsG38"},"source":["### Compare models without GA"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":2890,"status":"ok","timestamp":1671106162091,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"I9IBTPbfsjfq","outputId":"80c88d79-1f60-4520-bba3-9309e9525a40"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Classifier</th>\n","      <th>Accuracy</th>\n","      <th>Exec_Time_secs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>RandomForest</td>\n","      <td>0.800</td>\n","      <td>0.670056</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>MultinomialNB</td>\n","      <td>0.796</td>\n","      <td>0.003680</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LinearSVM</td>\n","      <td>0.780</td>\n","      <td>0.179135</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Logistic</td>\n","      <td>0.776</td>\n","      <td>0.133702</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>DecisionTree</td>\n","      <td>0.772</td>\n","      <td>0.092984</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>RadialSVM</td>\n","      <td>0.756</td>\n","      <td>0.170937</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>KNeighbors</td>\n","      <td>0.648</td>\n","      <td>0.000357</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Classifier  Accuracy  Exec_Time_secs\n","0   RandomForest     0.800        0.670056\n","1  MultinomialNB     0.796        0.003680\n","2      LinearSVM     0.780        0.179135\n","3       Logistic     0.776        0.133702\n","4   DecisionTree     0.772        0.092984\n","5      RadialSVM     0.756        0.170937\n","6     KNeighbors     0.648        0.000357"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["all_models_score_table = acc_score(X_bow, y_score)\n","all_models_score_table"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JGG-U4tCnzYz"},"source":["### Choosing the best classifier and starting evolution"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":497,"status":"ok","timestamp":1671106149258,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"BmuGlAV6nyEl"},"outputs":[],"source":["logmodel = RandomForestClassifier(n_estimators=200, random_state=0)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["selected_indexes, selected_terms = initial_population_term_selection_idf(idf, 2.7)\n","population_nextgen=generate_population(100, 100, 1553, selected_indexes)\n","scores, pop_after_fit = fitness_score(population_nextgen)\n","tf_idf_sent_score = term_frequency_inverse_document_frequency(pop_after_fit)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1611294,"status":"ok","timestamp":1671108227854,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"4wUYAM5aoegC","outputId":"dd2e3afe-e096-44ca-a48e-94863e0f4fc5"},"outputs":[],"source":["# st = time.time()\n","# chromo_set_1, score_set_1 = evolution(\n","#     size=100,\n","#     features_count=100,\n","#     chromo_size=X_bow.shape[1],\n","#     n_parents=80,\n","#     crossover_pb=0.8,\n","#     n_gen=100,\n","#     idf=idf, \n","#     idf_threshold=2.7,\n","#     tfidf_threshold=0.0240\n","# )\n","# et = time.time()\n","# exce_time_1 = et-st"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# import pickle\n","# with open('single_run_az_kbde.pkl', 'wb') as wf:\n","#     pickle.dump([chromo_set_1, score_set_1, exce_time_1], wf)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12642073,"status":"ok","timestamp":1671064702162,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"4x5yDEihQqZj","outputId":"9e195ee8-3142-4800-cc92-b4fd02acdac8"},"outputs":[],"source":["def run_n_evolution(n):\n","    result_n_runs = []\n","    for i in range(n):\n","        st = time.time()\n","        chromo_set_2, score_set_2 = evolution(\n","            size=100, \n","            features_count=100,\n","            chromo_size=X_bow.shape[1],\n","            n_parents=80,\n","            crossover_pb=0.7,\n","            n_gen=100,\n","            idf=idf, \n","            idf_threshold=2.7,\n","            tfidf_threshold=0.0240\n","        )\n","        et = time.time()\n","        result_n_runs.append((chromo_set_2, score_set_2, et-st))\n","    return result_n_runs"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best score in generation 1 : 0.696 feat_count: (100,)\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m run_n_evolution(\u001b[39m5\u001b[39;49m)\n","Cell \u001b[0;32mIn[37], line 5\u001b[0m, in \u001b[0;36mrun_n_evolution\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[1;32m      4\u001b[0m     st \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m     chromo_set_2, score_set_2 \u001b[39m=\u001b[39m evolution(\n\u001b[1;32m      6\u001b[0m         size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, \n\u001b[1;32m      7\u001b[0m         features_count\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m         chromo_size\u001b[39m=\u001b[39;49mX_bow\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m      9\u001b[0m         n_parents\u001b[39m=\u001b[39;49m\u001b[39m80\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m         crossover_pb\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m         n_gen\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m         idf\u001b[39m=\u001b[39;49midf, \n\u001b[1;32m     13\u001b[0m         idf_threshold\u001b[39m=\u001b[39;49m\u001b[39m2.7\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m         tfidf_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.0240\u001b[39;49m\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     et \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     17\u001b[0m     result_n_runs\u001b[39m.\u001b[39mappend((chromo_set_2, score_set_2, et\u001b[39m-\u001b[39mst))\n","Cell \u001b[0;32mIn[36], line 22\u001b[0m, in \u001b[0;36mevolution\u001b[0;34m(size, features_count, chromo_size, n_parents, crossover_pb, n_gen, idf, idf_threshold, tfidf_threshold)\u001b[0m\n\u001b[1;32m     19\u001b[0m     best_score\u001b[39m.\u001b[39mappend(scores[\u001b[39m0\u001b[39m])\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest score in generation\u001b[39m\u001b[39m'\u001b[39m,i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m,scores[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mfeat_count:\u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mwhere(pop_after_fit[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 22\u001b[0m     population_nextgen \u001b[39m=\u001b[39m de_mutation(population_nextgen\u001b[39m.\u001b[39;49mcopy(), crossover_pb, tfidf_threshold, n_parents)\n\u001b[1;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPopulation size:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(population_nextgen))\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m best_chromo,best_score\n","Cell \u001b[0;32mIn[35], line 66\u001b[0m, in \u001b[0;36mde_mutation\u001b[0;34m(pop_after_fit, co_probability, tfidf_threshold, n_parents)\u001b[0m\n\u001b[1;32m     63\u001b[0m     trail_vec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m i \u001b[39min\u001b[39;00m features \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(chromo_len)])\n\u001b[1;32m     64\u001b[0m     trail_vec \u001b[39m=\u001b[39m trail_vec\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint64)\n\u001b[0;32m---> 66\u001b[0m     new_trail \u001b[39m=\u001b[39m de_crossover(target_vec, trail_vec, co_probability)   \n\u001b[1;32m     68\u001b[0m     pop_nextgen\u001b[39m.\u001b[39mappend(new_trail)\n\u001b[1;32m     70\u001b[0m \u001b[39mreturn\u001b[39;00m pop_nextgen\n","Cell \u001b[0;32mIn[33], line 31\u001b[0m, in \u001b[0;36mde_crossover\u001b[0;34m(parent1, parent2, probability)\u001b[0m\n\u001b[1;32m     29\u001b[0m features\u001b[39m.\u001b[39msort()\n\u001b[1;32m     30\u001b[0m new_child \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m i \u001b[39min\u001b[39;00m features \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(chromo_len)])\n\u001b[0;32m---> 31\u001b[0m \u001b[39mreturn\u001b[39;00m de_fitness([new_child, parent_1])[\u001b[39m1\u001b[39m]\n","Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36mde_fitness\u001b[0;34m(chromo_set)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m chromo \u001b[39min\u001b[39;00m chromo_set:\n\u001b[1;32m      6\u001b[0m     features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(chromo\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m     logmodel\u001b[39m.\u001b[39;49mfit(X_train[:,features],Y_train)         \n\u001b[1;32m      8\u001b[0m     predictions \u001b[39m=\u001b[39m logmodel\u001b[39m.\u001b[39mpredict(X_test[:,features])\n\u001b[1;32m      9\u001b[0m     score \u001b[39m=\u001b[39m  accuracy_score(Y_test,predictions)\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    465\u001b[0m ]\n\u001b[1;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m )(\n\u001b[1;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    479\u001b[0m         t,\n\u001b[1;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[1;32m    481\u001b[0m         X,\n\u001b[1;32m    482\u001b[0m         y,\n\u001b[1;32m    483\u001b[0m         sample_weight,\n\u001b[1;32m    484\u001b[0m         i,\n\u001b[1;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    489\u001b[0m     )\n\u001b[1;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n","File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["results = run_n_evolution(5)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["import pickle\n","with open('n_run_az_kbde_v31.pkl', 'wb') as wf:\n","    pickle.dump(results, wf)"]}],"metadata":{"colab":{"collapsed_sections":["izg9ldpd-E7i","YPogbiu7QdPm","6UuM5nekVuo5","kF4wKdRcIIFE","9oEtXvs7jTJo","RFZbHNV6sIVP"],"provenance":[]},"kernelspec":{"display_name":"dev-ml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"vscode":{"interpreter":{"hash":"720be5bf9d441e2d6c30bd91b067816aa682de3307c54a83a56a5f6c3674f9d6"}}},"nbformat":4,"nbformat_minor":0}
