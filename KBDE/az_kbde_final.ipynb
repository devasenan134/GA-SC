{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":903,"status":"ok","timestamp":1671105990896,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"7ww5gjz_B-yc","outputId":"5abf9e18-cfeb-4231-ea76-9f55d9bc6e0d"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"izg9ldpd-E7i"},"source":["# **1. Pre-Processing**\n","1. Tokenization\n","2. Stemming/lemmatization\n","3. Bow/TF-IDF "]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671106094950,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"5o5kvRDQ8Wgf"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","import re\n","import numpy as np\n","\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671106094950,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"JABw2Ms1-EPn"},"outputs":[],"source":["def tokenize_lemmatizor(frame):\n","    words = []\n","    lemma_words = []\n","    lemma_sentences = []\n","    lemmatizer = WordNetLemmatizer()\n","\n","    for i in range(len(frame)):\n","        words = nltk.word_tokenize(frame.iloc[i])\n","        lemma_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n","        lemma_sentences.append(\" \".join(lemma_words))\n","\n","    return lemma_sentences\n","\n","\n","def re_lemmatizor(frame):\n","    lemmatizer = WordNetLemmatizer()\n","    review = []\n","    corpus = []\n","\n","    for i in range(len(frame)):\n","        review = re.sub('[^a-zA-Z]', ' ', frame.iloc[i])\n","        review = review.lower()\n","        review = review.split()\n","        # these lines represent - words = nltk.word_tokenize(frame.cmd[i])\n","\n","        review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n","        # lemma_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))])\n","        \n","        corpus.append(\" \".join(review))\n","        # lemma_sentences.append(\" \".join(lemma_words))\n","\n","    return corpus"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YPogbiu7QdPm"},"source":["### 3.1. BOW"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671106094950,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"-CMLKB7rRO5m"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer # bow\n","def tokens_to_bow(corpus_tokens, tokenizer=1):\n","    cv = CountVectorizer(max_features=5000)\n","    tokens = []\n","    if tokenizer == 1:\n","        tokens = tokenize_lemmatizor(corpus_tokens)\n","        X_bow = cv.fit_transform(tokens).toarray()\n","    else:\n","        tokens = re_lemmatizor(corpus_tokens)\n","        X_bow = cv.fit_transform(tokens).toarray()\n","    features = cv.get_feature_names_out()\n","    return X_bow, features"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6UuM5nekVuo5"},"source":["### 3.2. TF-IDF"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671106094951,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"5KBzYDsxV124"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer # tfidf\n","def tokens_to_tfidf(corpus_tokens, tokenizer=1):\n","    tfidf = TfidfVectorizer()\n","    tokens = []\n","    if tokenizer:\n","        tokens = tokenize_lemmatizor(corpus_tokens)\n","        X_tfidf = tfidf.fit_transform(tokens).toarray()\n","    else:\n","        tokens = re_lemmatizor(corpus_tokens)\n","        X_tfidf = tfidf.fit_transform(tokens).toarray()\n","    return X_tfidf, tokens"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9oEtXvs7jTJo"},"source":["## **Implementation**"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1671106092820,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"qatoafFKi_nY"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import time\n","from random import randint, choices, randrange, random, sample, shuffle\n","\n","\n","from sklearn import svm\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","from sklearn.naive_bayes import MultinomialNB\n","\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.model_selection import KFold, cross_val_score"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1671106092822,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"LzrikaHziawb"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","def split(df,label):\n","    X_tr, X_te, Y_tr, Y_te = train_test_split(df, label, test_size=0.25, random_state=42)\n","    return X_tr, X_te, Y_tr, Y_te"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1671106092822,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"RXX7NqZjhzne"},"outputs":[],"source":["classifiers = ['LinearSVM', 'RadialSVM', \n","               'Logistic',  'RandomForest', \n","               'DecisionTree', 'KNeighbors',\n","               'MultinomialNB']\n","\n","models = [svm.SVC(kernel='linear'),\n","          svm.SVC(kernel='rbf'),\n","          LogisticRegression(max_iter = 1000),\n","          RandomForestClassifier(n_estimators=200, random_state=0),\n","          DecisionTreeClassifier(random_state=0),\n","          KNeighborsClassifier(),\n","          MultinomialNB()]"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671106133007,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"HtqwQ-jziVs5"},"outputs":[],"source":["def acc_score(df,label):\n","    Score = pd.DataFrame({\"Classifier\":classifiers})\n","    j = 0\n","    acc = []\n","    exec_time = []\n","    X_train,X_test,Y_train,Y_test = split(df,label)\n","    for i in models:\n","        model = i\n","\n","        st = time.time()\n","        model.fit(X_train,Y_train)\n","        et = time.time()\n","\n","        predictions = model.predict(X_test)\n","        acc.append(accuracy_score(Y_test,predictions))\n","        exec_time.append(et-st)\n","        j = j+1     \n","    Score[\"Accuracy\"] = acc\n","    Score['Exec_Time_secs'] = exec_time\n","    Score.sort_values(by=\"Accuracy\", ascending=False,inplace = True)\n","    Score.reset_index(drop=True, inplace=True)\n","    return Score"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def initial_population_term_selection_idf(idf, idf_threshold):\n","    selected_indexes = []\n","    selected_terms = []\n","    idf_dict = dict(idf)\n","    for word, idf in idf_dict.items():\n","        if idf <= idf_threshold:\n","            selected_terms.append(word)\n","            selected_indexes.append(all_terms.index(word))\n","    return selected_indexes, selected_terms"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["len(initial_population_term_selection_idf(idf, 2.6)[1])\n","418\n","len(initial_population_term_selection_idf(idf, 2.7)[1])\n","651\n","len(initial_population_term_selection_idf(idf, 2.8)[1])\n","651\n","len(initial_population_term_selection_idf(idf, 2.9)[1])\n","651\n","len(initial_population_term_selection_idf(idf, 3)[1])\n","1553"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":544,"status":"ok","timestamp":1671106093348,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"_k_nxi6mEgPr"},"outputs":[],"source":["def generate_chromo(selected_indexes, features_count, chromo_size):\n","    features = sample(selected_indexes, k=features_count)\n","    features.sort()\n","    chromo = [1 if i in features else 0 for i in range(chromo_size)]\n","    return np.array(chromo)\n","\n","def generate_population(size, features_count, chromo_size, selected_indexes):\n","    return [generate_chromo(selected_indexes, features_count, chromo_size) for _ in range(size)]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def fitness_score(population):\n","    scores = []\n","    for chromosome in population:\n","        features = np.where(chromosome!=0)[0]\n","        logmodel.fit(X_train[:,features],Y_train)         \n","        predictions = logmodel.predict(X_test[:,features])\n","        scores.append(accuracy_score(Y_test,predictions))\n","    scores, population = np.array(scores), np.array(population) \n","    inds = np.argsort(scores)                                    \n","    return list(scores[inds][::-1]), list(population[inds,:][::-1])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def de_fitness(chromo_set):\n","    max_score = 0\n","    best_chromo = None\n","\n","    for chromo in chromo_set:\n","        features = np.where(chromo!=0)[0]\n","        logmodel.fit(X_train[:,features],Y_train)         \n","        predictions = logmodel.predict(X_test[:,features])\n","        score =  accuracy_score(Y_test,predictions)\n","        if max_score < score:\n","            max_score = score\n","            best_chromo = chromo\n","\n","    return max_score, best_chromo                                "]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def de_crossover(parent1, parent2, probability):\n","    child = []\n","    parent_1, parent_2 = parent1.copy(), parent2.copy()\n","    chromo_len = parent_1.shape[0]\n","    \n","    for i in range(chromo_len):\n","        if random() < probability:\n","            child.append(parent_1[i])\n","        else:\n","            child.append(parent_2[i])\n","    child = np.array(child)\n","    \n","    # there is randomization in this part, in future incase of any unexpected results, have to concentrate in this part\n","    features = np.where(child > 0)[0]\n","    non_features = np.where(child <= 0)[0]\n","\n","    # must_have_features = np.intersect1d(features, selected_indexes)\n","    # extra_features = np.setdiff1d(np.union1d(features, non_features), must_have_features)\n","    # # print(len(must_have_features))\n","    \n","    # features_count = len(must_have_features)\n","    # if len(must_have_features) > 100:\n","    #     to_remove = features_count - 100\n","    #     features = np.setdiff1d(must_have_features, sample(list(must_have_features), k=to_remove))\n","    # else:\n","    #     to_add = 100 - features_count\n","    #     features = np.append(must_have_features, sample(list(extra_features), k=to_add))\n","    \n","    features.sort()\n","    new_child = np.array([1 if i in features else 0 for i in range(chromo_len)])\n","    return de_fitness([new_child, parent_1])[1]"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def de_mutation(pop_after_fit, co_probability, f_score, tfidf_threshold, n_parents):\n","    # getting the population size\n","    pop_size = len(pop_after_fit)\n","    \n","    # getting the length of the chromosome\n","    chromo_len = len(pop_after_fit[0])\n","    # print(chromo_len)\n","\n","    # new variable for the mutated population\n","    pop_nextgen = []\n","\n","    # looping throught all the parent chromos in population\n","    for target in range(pop_size):\n","        # sample_space = list(range(pop_size))\n","        # sample_space.remove(target)\n","        \n","        tf_idf_sent_score = term_frequency_inverse_document_frequency(pop_after_fit)\n","        sent_indexes = [i for i, j in tf_idf_sent_score if (j <= tfidf_threshold) and (i != target)]\n","\n","        # sent_indexes = [tf_idf_sent_score[i][0] for i in range(len(tf_idf_sent_score)) if tf_idf_sent_score[i][0] != target]\n","        # sent_indexes = sent_indexes[:30]\n","        # sent_indexes.sort()\n","        # print(sent_indexes)\n","        # random selection of target chromo, and 2 random chromos\n","        rv1, rv2, rv3 = sample(sent_indexes, k=3)\n","        \n","        target_vec = pop_after_fit[target].astype(np.float32)\n","        random_vec1 = pop_after_fit[rv1]\n","        random_vec2 = pop_after_fit[rv2]\n","        random_vec3 = pop_after_fit[rv3]\n","\n","        # performing the DE mutation\n","        trail_vec = random_vec1 + f_score*(random_vec2-random_vec3)\n","        # x1_features = np.where(random_vec1 > 0)[0]\n","        # x2_features = np.where(random_vec2 > 0)[0]\n","        # x3_features = np.where(random_vec3 > 0)[0]\n","        \n","        # common_features = np.intersect1d(x1_features, x2_features)\n","        # common_features = np.intersect1d(common_features, x3_features)\n","        # print(common_features, len(common_features))\n","        \n","        # print(\"x1\", *random_vec1)\n","        # print(\"x2\", *(random_vec2-random_vec3))\n","        # print(\"u1\", *trail_vec)\n","        # print(len(np.where(trail_vec > 0)[0]))\n","        \n","        features = np.where(trail_vec > 0.5)[0]\n","        non_features = np.where(trail_vec <= 0.5)[0]\n","        \n","        #----------------------#\n","        # vec1_features = np.where(random_vec1 <= 0)[0]\n","        # not_imp_features = np.setdiff1d(features, vec1_features)\n","        \n","        # if len(features) > 100:\n","        #     to_remove = len(features) - 100\n","        #     features = np.setdiff1d(features, sample(list(not_imp_features), k=to_remove))\n","        # elif len(features) < 100:\n","        #     to_add = 100 - len(features)\n","        #     features = np.append(features, s  ample(list(vec1_features), k=to_add))\n","        #----------------------#\n","        \n","        features.sort()\n","        trail_vec = np.array([1 if i in features else 0 for i in range(chromo_len)])\n","        trail_vec = trail_vec.astype(np.int64)\n","        \n","        new_trail = de_crossover(target_vec, trail_vec, co_probability)   \n","        \n","        pop_nextgen.append(new_trail)\n","\n","    return pop_nextgen"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def term_frequency(population):\n","    tf_sent = []\n","    tf_dict = {}\n","    total_no_terms = len(population)\n","    for chromosome in population:\n","        chromo_tf = []\n","        indexes = np.where(chromosome!=0)\n","        for i in indexes[0]:\n","            chromo_tf.append(chromosome[i]/total_no_terms)\n","            tf_dict[all_terms[i]] = tf_dict.get(all_terms[i], 0) + (chromosome[i]/total_no_terms)\n","        tf_sent.append(chromo_tf)\n","    \n","    tf_terms = sorted(tf_dict.items(), key=lambda x: x[1], reverse=True)\n","    return tf_sent, tf_terms"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def inverse_document_frequency(population):\n","    idf = {}\n","    terms = np.array(list(all_terms))\n","    no_documents = len(population)\n","    for i in range(len(all_terms)):\n","        k = 0\n","        for chromosome in population:\n","            indexes = np.where(chromosome!=0)\n","            if terms[i] in terms[indexes]:\n","                k += 1\n","        idf[terms[i]] = np.log10(no_documents/k)\n","    idf = sorted(idf.items(), key=lambda x: x[1], reverse=True)\n","    return idf"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def term_frequency_inverse_document_frequency(population):\n","    tf_sent, tf_terms = term_frequency(population)\n","    tf_idf = {}\n","    idf_dict = dict(idf)\n","    for i in range(len(population)):\n","        tf_idf_sent = []\n","        indexes = np.where(population[i] != 0)[0]\n","        for j in range(len(indexes)):\n","            idf_term = idf_dict[all_terms[indexes[j]]]\n","            tf = tf_sent[i][j]\n","            tf_idf_sent.append(tf*idf_term)\n","        tf_idf[i] = sum(tf_idf_sent)/len(indexes)\n","    tf_idf = sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)\n","    return tf_idf"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1671106093351,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"jlLAoKgGs_to"},"outputs":[],"source":["def evolution(size, features_count, chromo_size,\n","            n_parents,\n","            crossover_pb,\n","            f_score,\n","            n_gen,\n","            idf, idf_threshold,\n","            tfidf_threshold\n","            ):\n","    best_chromo= []\n","    best_score= []\n","    \n","    selected_indexes, selected_terms = initial_population_term_selection_idf(idf, idf_threshold)\n","    population_nextgen=generate_population(size, features_count, chromo_size, selected_indexes)\n","    # scores, pop_after_fit = fitness_score(population_nextgen)\n","    # population_nextgen = pop_after_fit.copy()\n","\n","    for i in range(n_gen):\n","        scores, pop_after_fit = fitness_score(population_nextgen.copy())\n","        best_chromo.append(pop_after_fit[0])\n","        best_score.append(scores[0])\n","        print('Best score in generation',i+1,':',scores[0], \"feat_count:\", np.where(pop_after_fit[0] != 0)[0].shape)\n","        \n","        population_nextgen = de_mutation(population_nextgen.copy(), crossover_pb, f_score, tfidf_threshold, n_parents)\n","        \n","        print(\"Population size:\", len(population_nextgen))\n","        \n","    return best_chromo,best_score"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4RCS1eQ0G2Bn"},"source":["# **Accuracy Comparison**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RFZbHNV6sIVP"},"source":["### Data Preprocessing"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671106100902,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"5Pm8wDKNpdI3","outputId":"2daf50e3-6cc9-4559-b6e3-a222454c43e2"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cmd</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>So there is no way for me to plug it in here i...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Good case, Excellent value.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Great for the jawbone.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Tied to charger for conversations lasting more...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The mic is great.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>The screen does get smudged easily because it ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>What a piece of junk.. I lose more calls on th...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>Item Does Not Match Picture.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>The only thing that disappoint me is the infra...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>You can not answer calls with the unit, never ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   cmd  score\n","0    So there is no way for me to plug it in here i...      0\n","1                          Good case, Excellent value.      1\n","2                               Great for the jawbone.      1\n","3    Tied to charger for conversations lasting more...      0\n","4                                    The mic is great.      1\n","..                                                 ...    ...\n","995  The screen does get smudged easily because it ...      0\n","996  What a piece of junk.. I lose more calls on th...      0\n","997                       Item Does Not Match Picture.      0\n","998  The only thing that disappoint me is the infra...      0\n","999  You can not answer calls with the unit, never ...      0\n","\n","[1000 rows x 2 columns]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["amazon = pd.read_csv(\"../dataset/amazon.csv\")\n","frame = amazon.copy()\n","frame"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1766,"status":"ok","timestamp":1671106102663,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"8vV0bqNbrkxk","outputId":"6707bf88-6b91-4616-8f2a-8eacdb2833d1"},"outputs":[],"source":["X_bow, features = tokens_to_bow(frame.cmd, 0)\n","y_score = frame.score\n","all_terms = list(features)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["X_train, X_test, Y_train, Y_test = split(X_bow, y_score)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["idf = inverse_document_frequency(X_bow)\n","tf_sent, tf_terms = term_frequency(X_bow)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZoErA4fHsG38"},"source":["### Compare models without GA"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":2890,"status":"ok","timestamp":1671106162091,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"I9IBTPbfsjfq","outputId":"80c88d79-1f60-4520-bba3-9309e9525a40"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Classifier</th>\n","      <th>Accuracy</th>\n","      <th>Exec_Time_secs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>RandomForest</td>\n","      <td>0.800</td>\n","      <td>1.277574</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>MultinomialNB</td>\n","      <td>0.796</td>\n","      <td>0.008976</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LinearSVM</td>\n","      <td>0.780</td>\n","      <td>0.544539</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Logistic</td>\n","      <td>0.776</td>\n","      <td>0.075798</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>DecisionTree</td>\n","      <td>0.772</td>\n","      <td>0.200463</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>RadialSVM</td>\n","      <td>0.756</td>\n","      <td>0.665214</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>KNeighbors</td>\n","      <td>0.648</td>\n","      <td>0.001000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Classifier  Accuracy  Exec_Time_secs\n","0   RandomForest     0.800        1.277574\n","1  MultinomialNB     0.796        0.008976\n","2      LinearSVM     0.780        0.544539\n","3       Logistic     0.776        0.075798\n","4   DecisionTree     0.772        0.200463\n","5      RadialSVM     0.756        0.665214\n","6     KNeighbors     0.648        0.001000"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["all_models_score_table = acc_score(X_bow, y_score)\n","all_models_score_table"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JGG-U4tCnzYz"},"source":["### Choosing the best classifier and starting evolution"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":497,"status":"ok","timestamp":1671106149258,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"BmuGlAV6nyEl"},"outputs":[],"source":["logmodel = RandomForestClassifier(n_estimators=200, random_state=0)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["selected_indexes, selected_terms = initial_population_term_selection_idf(idf, 2.7)\n","population_nextgen=generate_population(100, 100, 1553, selected_indexes)\n","scores, pop_after_fit = fitness_score(population_nextgen)\n","tf_idf_sent_score = term_frequency_inverse_document_frequency(pop_after_fit)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1611294,"status":"ok","timestamp":1671108227854,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"4wUYAM5aoegC","outputId":"dd2e3afe-e096-44ca-a48e-94863e0f4fc5"},"outputs":[],"source":["# st = time.time()\n","# chromo_set_1, score_set_1 = evolution(\n","#     size=100,\n","#     features_count=100,\n","#     chromo_size=X_bow.shape[1],\n","#     n_parents=80,\n","#     crossover_pb=0.8,\n","#     n_gen=100,\n","#     idf=idf, \n","#     idf_threshold=2.7,\n","#     tfidf_threshold=0.0240\n","# )\n","# et = time.time()\n","# exce_time_1 = et-st"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# import pickle\n","# with open('single_run_az_kbde.pkl', 'wb') as wf:\n","#     pickle.dump([chromo_set_1, score_set_1, exce_time_1], wf)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12642073,"status":"ok","timestamp":1671064702162,"user":{"displayName":"Devasenan Murugan","userId":"15686758824548239314"},"user_tz":-330},"id":"4x5yDEihQqZj","outputId":"9e195ee8-3142-4800-cc92-b4fd02acdac8"},"outputs":[],"source":["def run_n_evolution(n):\n","    result_n_runs = []\n","    for i in range(n):\n","        st = time.time()\n","        chromo_set_2, score_set_2 = evolution(\n","            size=100, \n","            features_count=100,\n","            chromo_size=X_bow.shape[1],\n","            n_parents=80,\n","            crossover_pb=0.4,\n","            f_score=0.4,\n","            n_gen=100,\n","            idf=idf, \n","            idf_threshold=2.7,\n","            tfidf_threshold=0.0240\n","        )\n","        et = time.time()\n","        result_n_runs.append((chromo_set_2, score_set_2, et-st))\n","    return result_n_runs"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best score in generation 1 : 0.688 feat_count: (100,)\n"]}],"source":["results = run_n_evolution(30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","with open('30_run_az_kbde_pt_final_04.pkl', 'wb') as wf:\n","    pickle.dump(results, wf)"]}],"metadata":{"colab":{"collapsed_sections":["izg9ldpd-E7i","YPogbiu7QdPm","6UuM5nekVuo5","kF4wKdRcIIFE","9oEtXvs7jTJo","RFZbHNV6sIVP"],"provenance":[]},"kernelspec":{"display_name":"dev-ml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"720be5bf9d441e2d6c30bd91b067816aa682de3307c54a83a56a5f6c3674f9d6"}}},"nbformat":4,"nbformat_minor":0}
