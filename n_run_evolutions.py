# -*- coding: utf-8 -*-
"""n_run_evolutions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qAiKoxiD3RWPAGPTkZ-BaGu5DnYyY0eV
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

"""# **1. Pre-Processing**
1. Tokenization
2. Stemming/lemmatization
3. Bow/TF-IDF 
"""

from nltk.stem import WordNetLemmatizer
import re
import numpy as np

from nltk.stem import PorterStemmer
from nltk.corpus import stopwords

def tokenize_lemmatizor(frame):
    words = []
    lemma_words = []
    lemma_sentences = []
    lemmatizer = WordNetLemmatizer()

    for i in range(len(frame)):
        words = nltk.word_tokenize(frame.iloc[i])
        lemma_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]
        lemma_sentences.append(" ".join(lemma_words))

    return lemma_sentences

def re_lemmatizor(frame):
    lemmatizer = WordNetLemmatizer()
    review = []
    corpus = []

    for i in range(len(frame)):
        review = re.sub('[^a-zA-Z]', ' ', frame.iloc[i])
        review = review.lower()
        review = review.split()
        # these lines represent - words = nltk.word_tokenize(frame.cmd[i])

        review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
        # lemma_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))])
        
        corpus.append(" ".join(review))
        # lemma_sentences.append(" ".join(lemma_words))

    return corpus

"""### 3.1. BOW"""

from sklearn.feature_extraction.text import CountVectorizer # bow
def tokens_to_bow(corpus_tokens, tokenizer=1):
    cv = CountVectorizer(max_features=5000)
    if tokenizer:
        X_bow = cv.fit_transform(tokenize_lemmatizor(corpus_tokens)).toarray()
    else:
        X_bow = cv.fit_transform(re_lemmatizor(corpus_tokens)).toarray()
    return X_bow

"""### 3.2. TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer # tfidf
def tokens_to_tfidf(corpus_tokens, tokenizer=1):
    tfidf = TfidfVectorizer()
    if tokenizer:
        X_tfidf = tfidf.fit_transform(tokenize_lemmatizor(corpus_tokens)).toarray()
    else:
        X_tfidf = tfidf.fit_transform(re_lemmatizor(corpus_tokens)).toarray()
    return X_tfidf

"""## **Real Time preprocessing**"""

# def real_time_preprocessing(sentence):
#     # these lines represent - words = nltk.word_tokenize(frame.cmd[i])
#     review = re.sub('[^a-zA-Z]', ' ', sentence)
#     review = review.lower()
#     review = review.split()

#     # lemma_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))])
#     lemmatizer = WordNetLemmatizer()
#     review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    
#     review_tokens = " ".join(review)

#     X_vector = vectorizer.transform([review_tokens]).toarray()
 
#     return X_vector

"""# **Dummy**"""

# dummy test practice
bow = [[0, 1, 0, 1, 1],
       [1, 0, 1, 0, 0],
       [0, 1, 1, 1, 0],
       [1, 0, 0, 0, 1],
       [0, 1, 0, 0, 0],
       [1, 1, 0, 0, 1]]
bow = np.array(bow)
bow

bow[:, [1,0,1,0,1]]

"""# **2. Feature Extraction using GA**


1. genetic representation of a solution
2. a function to generate new solutions
3. fitness function
4. selection function
5. crossover function
6. mutation function





*   Generate Possible set of sollutions called the generation
*   fitness function to test, how good a solution is - resembles natural selection
*   parents are selected for next generation of solutions
*   cross over functions are used on parents to generate 2 new solutions of next gen
*   mutation

## **Implementation**

enkita ippo bow la produce panna matrix iruku

so ippo i have to iterate through matrix by each row (means each sentence)

and in the fitness function,
each row la active ah iruka words ah vechi
antha features (means bow la that words) ah mattum extract panni
antha extracted features ah oru np array ah convert panni
then model la kuduthu train and test panni

vara accuracy ah use panni next generation of features ah select pannikanum



hope this flow works :)
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import random
import time
from random import randint, choices, randrange, random, sample

from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.naive_bayes import MultinomialNB

from sklearn import metrics
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import KFold, cross_val_score

from sklearn.model_selection import train_test_split
def split(df,label):
    X_tr, X_te, Y_tr, Y_te = train_test_split(df, label, test_size=0.25, random_state=42)
    return X_tr, X_te, Y_tr, Y_te

classifiers = ['LinearSVM', 'RadialSVM', 
               'Logistic',  'RandomForest', 
               'DecisionTree', 'KNeighbors',
               'MultinomialNB']

models = [svm.SVC(kernel='linear'),
          svm.SVC(kernel='rbf'),
          LogisticRegression(max_iter = 1000),
          RandomForestClassifier(n_estimators=200, random_state=0),
          DecisionTreeClassifier(random_state=0),
          KNeighborsClassifier(),
          MultinomialNB()]

def acc_score(df,label):
    Score = pd.DataFrame({"Classifier":classifiers})
    j = 0
    acc = []
    exec_time = []
    X_train,X_test,Y_train,Y_test = split(df,label)
    for i in models:
        model = i

        st = time.time()
        model.fit(X_train,Y_train)
        et = time.time()

        predictions = model.predict(X_test)
        acc.append(accuracy_score(Y_test,predictions))
        exec_time.append(et-st)
        j = j+1     
    Score["Accuracy"] = acc
    Score['Exec_Time_secs'] = exec_time
    Score.sort_values(by="Accuracy", ascending=False,inplace = True)
    Score.reset_index(drop=True, inplace=True)
    return Score

def plot(score,x,y,c = "b"):
    gen = [1,2,3,4,5]
    plt.figure(figsize=(6,4))
    ax = sns.pointplot(x=gen, y=score,color = c )
    ax.set(xlabel="Generation", ylabel="Accuracy")
    ax.set(ylim=(x,y))

def generate_chromo(features_count, length):
    features = sample(range(length), k=features_count)
    features.sort()
    chromo = [1 if i in features else 0 for i in range(length)]
    return chromo

def generate_population(size, features_count, length):
    return np.array([generate_chromo(features_count, length) for _ in range(size)])

def feature_selection(sent):
    indexes = []
    for i in range(len(sent)):
        if sent[i] >= 1:
            indexes.append(i)
    return indexes

def single_point_crossover(pop_after_sel, probability):
    pop_nextgen = list(pop_after_sel)
    length = len(pop_nextgen)
    chrom_l = len(pop_nextgen[0])
    for i in range(0, len(pop_after_sel), 2):
        new_chromo = []
        parent_1, parent_2 = pop_nextgen[i], pop_nextgen[i+1]
        if random() < probability:
            k = randint(1, chrom_l - 1)  # crossover_point
            new_child = np.concatenate([parent_1[:k], parent_2[k:]])
            pop_nextgen.append(new_child)
    return pop_nextgen

# single_point_crossover(bow, 0.5)

def bit_flip_mutation(pop_after_cross, probability, mutation_rate,n_feat):   
    mutation_range = int(mutation_rate*n_feat)
    pop_next_gen = []
    for n in range(0,len(pop_after_cross)):
        chromo = pop_after_cross[n]
        rand_posi = []
        if random() > probability:
            pop_next_gen.append(chromo)
        else:
            for i in range(0,mutation_range):
                pos = randint(0,n_feat-1)
                rand_posi.append(pos)
            for j in rand_posi:
                chromo[j] = abs(chromo[j] - 1)
            pop_next_gen.append(chromo)
    return pop_next_gen

def population_selection(pop_after_fit,n_parents):
    population_nextgen = []
    for i in range(n_parents):
        population_nextgen.append(pop_after_fit[i])
    return population_nextgen

def fitness_score(population):
    scores = []
    for chromosome in population:
        features = feature_selection(chromosome)
        logmodel.fit(X_train[:,features],Y_train)         
        predictions = logmodel.predict(X_test[:,features])
        scores.append(accuracy_score(Y_test,predictions))
    scores, population = np.array(scores), np.array(population) 
    inds = np.argsort(scores)                                    
    return list(scores[inds][::-1]), list(population[inds,:][::-1])

def evolution(df,
            label,
            size, feat_count, n_feat,
            n_parents,
            crossover_pb, mutation_pb,
            mutation_rate,
            n_gen,
            X_train, X_test, Y_train, Y_test):
    best_chromo= []
    best_score= []
    population_nextgen=generate_population(size, feat_count, n_feat)
    for i in range(n_gen):
        scores, pop_after_fit = fitness_score(population_nextgen)
        print('Best score in generation',i+1,':',scores[:1])
        pop_after_sel = population_selection(pop_after_fit,n_parents)
        pop_after_cross = single_point_crossover(pop_after_sel, crossover_pb)
        population_nextgen = bit_flip_mutation(pop_after_cross, mutation_pb, mutation_rate, n_feat)
        best_chromo.append(pop_after_fit[0])
        best_score.append(scores[0])
    return best_chromo,best_score

np.concatenate(([1,2,3,4],[5,6,7,8]))

"""# **Accuracy Comparison**

## **For Amazon dataset**

### Data Preprocessing
"""

amazon = pd.read_csv("data/amazon.csv")
amazon

frame = amazon.copy()

"""*Difference between a stemmed and lemmatized words*"""


X_bow = tokens_to_bow(frame.cmd, 0)
y_score = frame.score
X_bow.shape

"""### Compare models without GA"""

score1 = acc_score(X_bow, y_score)
score1

"""### Choosing the best classifier and starting evolution"""

logmodel = RandomForestClassifier(n_estimators=200, random_state=0)

X_train, X_test, Y_train, Y_test = split(X_bow, y_score)

"""### Trying different parameters"""

# chromo_set_1, score_set_1 = evolution(
#     X_bow,
#     y_score,
#     size=80, 
#     feat_count=100,
#     n_feat=X_bow.shape[1],
#     n_parents=64,
#     crossover_pb=0.5,
#     mutation_pb=0.04,
#     mutation_rate=0.2,
#     n_gen=30,
#     X_train = X_train,
#     X_test = X_test,
#     Y_train = Y_train,
#     Y_test = Y_test
# )

# import pickle
# with open('single_run.pkl', 'wb') as wf:
#     pickle.dump([chromo_set_1, score_set_1], wf)

def run_n_evolution(n):
    result_n_runs = []
    for i in range(n):
        chromo_set_2, score_set_2 = evolution(
            X_bow,
            y_score,
            size=80, 
            feat_count=100,
            n_feat=X_bow.shape[1],
            n_parents=64,
            crossover_pb=0.5,
            mutation_pb=0.04,
            mutation_rate=0.2,
            n_gen=30,
            X_train = X_train,
            X_test = X_test,
            Y_train = Y_train,
            Y_test = Y_test
        )
        result_n_runs.append((chromo_set_2, score_set_2))
    return result_n_runs

# plot(score_set_2, 0.5, 1.0, c = "green")