# -*- coding: utf-8 -*-
"""n_run_evolutions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qAiKoxiD3RWPAGPTkZ-BaGu5DnYyY0eV
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

"""# **1. Pre-Processing**
1. Tokenization
2. Stemming/lemmatization
3. Bow/TF-IDF 
"""

from nltk.stem import WordNetLemmatizer
import re
import numpy as np

from nltk.stem import PorterStemmer
from nltk.corpus import stopwords

def tokenize_lemmatizor(frame):
    words = []
    lemma_words = []
    lemma_sentences = []
    lemmatizer = WordNetLemmatizer()

    for i in range(len(frame)):
        words = nltk.word_tokenize(frame.iloc[i])
        lemma_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]
        lemma_sentences.append(" ".join(lemma_words))

    return lemma_sentences

def re_lemmatizor(frame):
    lemmatizer = WordNetLemmatizer()
    review = []
    corpus = []

    for i in range(len(frame)):
        review = re.sub('[^a-zA-Z]', ' ', frame.iloc[i])
        review = review.lower()
        review = review.split()
        # these lines represent - words = nltk.word_tokenize(frame.cmd[i])

        review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
        # lemma_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))])
        
        corpus.append(" ".join(review))
        # lemma_sentences.append(" ".join(lemma_words))

    return corpus

"""### 3.1. BOW"""

from sklearn.feature_extraction.text import CountVectorizer # bow
def tokens_to_bow(corpus_tokens, tokenizer=1):
    cv = CountVectorizer(max_features=5000)
    tokens = []
    if tokenizer == 1:
        tokens = tokenize_lemmatizor(corpus_tokens)
        X_bow = cv.fit_transform(tokens).toarray()
    else:
        tokens = re_lemmatizor(corpus_tokens)
        X_bow = cv.fit_transform(tokens).toarray()
    features = cv.get_feature_names_out()
    return X_bow, features

"""### 3.2. TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer # tfidf
def tokens_to_tfidf(corpus_tokens, tokenizer=1):
    tfidf = TfidfVectorizer()
    tokens = []
    if tokenizer:
        tokens = tokenize_lemmatizor(corpus_tokens)
        X_tfidf = tfidf.fit_transform(tokens).toarray()
    else:
        tokens = re_lemmatizor(corpus_tokens)
        X_tfidf = tfidf.fit_transform(tokens).toarray()
    return X_tfidf, tokens

"""## **Real Time preprocessing**"""

# def real_time_preprocessing(sentence):
#     # these lines represent - words = nltk.word_tokenize(frame.cmd[i])
#     review = re.sub('[^a-zA-Z]', ' ', sentence)
#     review = review.lower()
#     review = review.split()

#     # lemma_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))])
#     lemmatizer = WordNetLemmatizer()
#     review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
    
#     review_tokens = " ".join(review)

#     X_vector = vectorizer.transform([review_tokens]).toarray()
 
#     return X_vector


"""# **2. Feature Extraction using GA**


1. genetic representation of a solution
2. a function to generate new solutions
3. fitness function
4. selection function
5. crossover function
6. mutation function





*   Generate Possible set of sollutions called the generation
*   fitness function to test, how good a solution is - resembles natural selection
*   parents are selected for next generation of solutions
*   cross over functions are used on parents to generate 2 new solutions of next gen
*   mutation

## **Implementation**

enkita ippo bow la produce panna matrix iruku

so ippo i have to iterate through matrix by each row (means each sentence)

and in the fitness function,
each row la active ah iruka words ah vechi
antha features (means bow la that words) ah mattum extract panni
antha extracted features ah oru np array ah convert panni
then model la kuduthu train and test panni

vara accuracy ah use panni next generation of features ah select pannikanum



hope this flow works :)
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import random
import time
from random import randint, choices, randrange, random, sample, shuffle

from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.naive_bayes import MultinomialNB

from sklearn import metrics
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import KFold, cross_val_score

from sklearn.model_selection import train_test_split
def split(df,label):
    X_tr, X_te, Y_tr, Y_te = train_test_split(df, label, test_size=0.25, random_state=42)
    return X_tr, X_te, Y_tr, Y_te

classifiers = ['LinearSVM', 'RadialSVM', 
               'Logistic',  'RandomForest', 
               'DecisionTree', 'KNeighbors',
               'MultinomialNB']

models = [svm.SVC(kernel='linear'),
          svm.SVC(kernel='rbf'),
          LogisticRegression(max_iter = 1000),
          RandomForestClassifier(n_estimators=200, random_state=0),
          DecisionTreeClassifier(random_state=0),
          KNeighborsClassifier(),
          MultinomialNB()]

def acc_score(df,label):
    Score = pd.DataFrame({"Classifier":classifiers})
    j = 0
    acc = []
    exec_time = []
    X_train,X_test,Y_train,Y_test = split(df,label)
    for i in models:
        model = i

        st = time.time()
        model.fit(X_train,Y_train)
        et = time.time()

        predictions = model.predict(X_test)
        acc.append(accuracy_score(Y_test,predictions))
        exec_time.append(et-st)
        j = j+1     
    Score["Accuracy"] = acc
    Score['Exec_Time_secs'] = exec_time
    Score.sort_values(by="Accuracy", ascending=False,inplace = True)
    Score.reset_index(drop=True, inplace=True)
    return Score

def plot(score,x,y,c = "b"):
    gen = [1,2,3,4,5]
    plt.figure(figsize=(6,4))
    ax = sns.pointplot(x=gen, y=score,color = c )
    ax.set(xlabel="Generation", ylabel="Accuracy")
    ax.set(ylim=(x,y))

def generate_chromo(features_count, chromo_size):
    features = sample(range(chromo_size), k=features_count)
    features.sort()
    chromo = [1 if i in features else 0 for i in range(chromo_size)]
    return np.array(chromo)

def generate_population(size, features_count, chromo_size):
    return [generate_chromo(features_count, chromo_size) for _ in range(size)]

def single_point_crossover(pop_after_sel, probability):
    shuffle(list(pop_after_sel))
    pop_nextgen = pop_after_sel
    length = len(pop_nextgen)
    chrom_l = len(pop_nextgen[0])
    for i in range(0, len(pop_after_sel), 2):
        parent_1, parent_2 = pop_nextgen[i], pop_nextgen[i+1]
        if random() <= probability:
            k = randint(1, chrom_l - 1)  # crossover_point
            new_child_1 = np.concatenate([parent_1[:k], parent_2[k:]])
            new_child_2 = np.concatenate([parent_2[:k], parent_1[k:]])
            pop_nextgen.append(new_child_1)
            pop_nextgen.append(new_child_2)
            # print("co c1:", np.where(new_child_1 != 0)[0].shape)
            # print("co c2:", np.where(new_child_2 != 0)[0].shape)
        else:
            pop_nextgen.append(parent_1)
            pop_nextgen.append(parent_2)
            # print("co p1:", np.where(parent_1 != 0)[0].shape)
            # print("co p2:", np.where(parent_2 != 0)[0].shape)
        
    return pop_nextgen

# single_point_crossover(bow, 0.5)

def single_point_crossover1(pop_after_sel, probability, n_parents):
    shuffle(list(pop_after_sel))
    pop_nextgen = pop_after_sel.copy()
    chromo_l = len(pop_nextgen[0])
    for i in range(0, len(pop_after_sel), 2):
        parent_1, parent_2 = pop_nextgen[i].copy(), pop_nextgen[i+1].copy()
        p1_features = list(np.where(np.array(parent_1) != 0)[0])
        p2_features = list(np.where(np.array(parent_2) != 0)[0])
        if random() <= probability:
            k = randint(0,len(p1_features))  # crossover_point
            c1_features = p1_features[:k] + p2_features[k:]
            c2_features = p2_features[:k] + p1_features[k:]

            c1_dup = list(set([i for i in c1_features if c1_features.count(i) > 1]))
            c2_dup = list(set([i for i in c2_features if c2_features.count(i) > 1]))
            # print("duplicates:", c1_dup, c2_dup)

            if len(c1_dup) > 0:
                sample_pop1 = [i for i in p1_features if i not in c1_features]
                k1 = sample(sample_pop1, k=len(c1_dup))
                for i in c1_dup:
                    c1_features.remove(i)
                c1_features.extend(k1)
            elif len(c2_dup) > 0:
                sample_pop2 = [i for i in p2_features if i not in c2_features]
                k2 = sample(sample_pop2, k=len(c2_dup))
                for i in c2_dup:
                    c2_features.remove(i)
                c2_features.extend(k2)

            new_child_1 = np.array([1 if i in c1_features else 0 for i in range(chromo_l)])
            new_child_2 = np.array([1 if i in c2_features else 0 for i in range(chromo_l)])
            pop_nextgen.append(new_child_1)
            pop_nextgen.append(new_child_2)
            # print(len(c1_features), len(c2_features))
        else:
            pop_nextgen.append(np.array(parent_1))
            pop_nextgen.append(np.array(parent_2))
        
    _, pop_nextgen = fitness_score(pop_nextgen)
    return pop_nextgen[:n_parents]

def bit_flip_mutation(pop_after_cross, probability, mutation_rate):
    n_feat = pop_after_cross[0].shape[0]   
    mutation_range = int(mutation_rate*n_feat)
    pop_next_gen = []
    for n in range(len(pop_after_cross)):
        chromo = pop_after_cross[n]
        rand_posi = []
        if random() <= probability:
            for i in range(mutation_range):
                pos = randint(0,n_feat-1)
                rand_posi.append(pos)
            for j in rand_posi:
                chromo[j] = abs(chromo[j] - 1)
        pop_next_gen.append(chromo)
        # print("MU ch:", np.where(chromo != 0)[0].shape)
    return pop_next_gen

def bit_flip_mutation1(pop_after_cross, probability, mutation_rate, features_count, n_parents):
    mutation_range = int(mutation_rate*features_count)
    # if mutation_range%2 != 0:
    #     mutation_range -= 1
    # mid = mutation_range//2
    # print(mutation_range, features_count)
    
    pop_next_gen = pop_after_cross.copy()

    for n in range(len(pop_after_cross)):
        chromo = pop_after_cross[n].copy()
        features = list(np.where(chromo != 0)[0])
        non_features = list(np.setdiff1d(np.array(range(chromo.shape[0])), features))
        
        rand_posi = []
        if random() <= probability:
            features_pos = sample(features, k=mutation_range)
            non_features_pos = sample(non_features, k=mutation_range)
            rand_posi.extend(features_pos)
            rand_posi.extend(non_features_pos)
            for j in rand_posi:
                chromo[j] = abs(chromo[j] - 1)
        pop_next_gen.append(chromo)
    
    _, pop_next_gen = fitness_score(pop_next_gen)
    return pop_next_gen[:n_parents+20], _

def population_selection(pop_after_fit,n_parents):
    # size = int(len(pop_after_fit)*(n_parents/100))
    population_nextgen = []
    for i in range(n_parents):
        population_nextgen.append(pop_after_fit[i])
    return population_nextgen

def fitness_score(population):
    scores = []
    for chromosome in population:
        features = np.where(chromosome!=0)[0]
        logmodel.fit(X_train[:,features],Y_train)         
        predictions = logmodel.predict(X_test[:,features])
        scores.append(accuracy_score(Y_test,predictions))
    scores, population = np.array(scores), np.array(population) 
    inds = np.argsort(scores)                                    
    return list(scores[inds][::-1]), list(population[inds,:][::-1])

def evolution(size, features_count, chromo_size,
            n_parents,
            crossover_pb, mutation_pb,
            mutation_rate,
            n_gen):
    best_chromo= []
    best_score= []
    
    population_nextgen=generate_population(size, features_count, chromo_size)
    scores, pop_after_fit = fitness_score(population_nextgen)
    population_nextgen = pop_after_fit.copy()

    for i in range(n_gen):
        pop_after_fit = population_nextgen.copy()
        best_chromo.append(pop_after_fit[0])
        best_score.append(scores[0])
        print('Best score in generation',i+1,':',scores[0], "feat_count:", np.where(pop_after_fit[0] != 0)[0].shape)

        pop_after_sel = population_selection(pop_after_fit, n_parents)
        
        pop_after_cross = single_point_crossover1(pop_after_sel, crossover_pb, n_parents)

        population_nextgen, scores = bit_flip_mutation1(pop_after_cross, mutation_pb, mutation_rate, features_count, n_parents)

        # new next gen population will have the evolved population + the initial population after fitness_score
        # _, population_new_nextgen = fitness_score(population_nextgen)
        # population_nextgen = population_selection(population_new_nextgen, n_parents)
        print("Population size:", len(population_nextgen))
        
    return best_chromo,best_score

"""# **Accuracy Comparison**

## **For Amazon dataset**

### Data Preprocessing
"""

def initialize_base_variables(dataset_path):
    amazon = pd.read_csv(dataset_path)
    amazon

    frame = amazon.copy()

    """*Difference between a stemmed and lemmatized words*"""


    X_bow, features = tokens_to_bow(frame.cmd, 0)
    y_score = frame.score
    all_terms = list(features)

    X_train, X_test, Y_train, Y_test = split(X_bow, y_score)

    """### Compare models without GA"""
    all_models_score_table = acc_score(X_bow, y_score)
    all_models_score_table


    """### Choosing the best classifier and starting evolution"""
    logmodel = RandomForestClassifier(n_estimators=200, random_state=0)

    return X_bow, y_score, logmodel

"""### Trying different parameters"""

# st = time.time()
# chromo_set_1, score_set_1 = evolution(
#     size=100,
#     features_count=100,
#     chromo_size=X_bow.shape[1],
#     n_parents=80,
#     crossover_pb=0.8,
#     mutation_pb=0.05,
#     mutation_rate=0.05,
#     n_gen=100
# )
# et = time.time()
# exce_time_1 = et-st

# sns.lineplot(x=list(range(1, 101)), y=score_set_1)
# plt.title('Amazon Dataset')
# plt.xlabel('Generations')
# plt.ylabel('Accuracy')

# import pickle
# with open('single_run_az_100fc.pkl', 'wb') as wf:
#     pickle.dump([chromo_set_1, score_set_1, exce_time_1], wf)

# def run_n_evolution(n):
#     result_n_runs = []
#     for i in range(n):
#         st = time.time()
#         chromo_set_2, score_set_2 = evolution(
#             size=100, 
#             features_count=100,
#             chromo_size=X_bow.shape[1],
#             n_parents=80,
#             crossover_pb=0.8,
#             mutation_pb=0.05,
#             mutation_rate=0.05,
#             n_gen=100
#         )
#         et = time.time()
#         result_n_runs.append((chromo_set_2, score_set_2, et-st))
#     return result_n_runs
# plot(score_set_2, 0.5, 1.0, c = "green")

# results = run_n_evolution(30)

# import pickle
# with open('n_run_az_100fc.pkl', 'wb') as wf:
#     pickle.dump(results, wf)